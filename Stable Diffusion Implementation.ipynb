{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0a9344-de43-4e97-b928-bce1b4ef6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/resnet.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.groupnorm_1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.groupnorm_2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.proj_input = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        else:\n",
    "            self.proj_input = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_norm = self.groupnorm_1(x)\n",
    "        out = F.silu(x_norm)\n",
    "        out = self.conv_1(out)\n",
    "\n",
    "        out = self.groupnorm_2(out)\n",
    "        out = F.silu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv_2(out)\n",
    "        \n",
    "        return out + self.proj_input(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff97a0d-cb1a-483d-904f-1b856fe6f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/vae.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/vae.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from .resnet import ResidualBlock\n",
    "from .attention import MultiheadSelfAttention\n",
    "from typing import List\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.pad = (0, 1, 0, 1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.pad(x, self.pad)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.attn = MultiheadSelfAttention(num_heads=1, embedding_dim=in_channels)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (n, c, h, w)\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        x_norm = self.groupnorm(x)\n",
    "        # (n, c, h, w) -> (n, c, h * w) -> (n, h * w, c)\n",
    "        x_norm = x_norm.view((batch_size, channels, -1)).transpose(1, 2)\n",
    "\n",
    "        # (n, h * w, c)\n",
    "        out = self.attn(x=x_norm)\n",
    "\n",
    "        # (n, h * w, c) -> (n, c, h * w) -> (n, c, h, w)\n",
    "        out = out.transpose(1, 2).reshape(x.shape)\n",
    "        \n",
    "        return out + x\n",
    "        \n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int, ch_mult: List[int]=[1, 2, 4, 8], dropout: float=0.0, z_channels: int=8):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # start\n",
    "        self.down = nn.ModuleList()\n",
    "        in_ch_mult = [1] + ch_mult\n",
    "        ch = 128\n",
    "        for i in range(len(ch_mult)):\n",
    "            block_in = ch * in_ch_mult[i]\n",
    "            block_out = ch * ch_mult[i]\n",
    "            block = nn.Sequential(\n",
    "                ResidualBlock(block_in, block_out, dropout),\n",
    "                ResidualBlock(block_out, block_out, dropout),\n",
    "            )\n",
    "            \n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            if i != len(ch_mult) - 1:\n",
    "                down.downsample = Downsample(block_out)\n",
    "            else:\n",
    "                down.downsample = nn.Identity()\n",
    "                \n",
    "            self.down.append(down)\n",
    "            curr_channels = block_out\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.res_block_1 = ResidualBlock(curr_channels, curr_channels)\n",
    "        self.mid.attn_block_1 = AttentionBlock(in_channels=curr_channels)\n",
    "        self.mid.res_block_2 = ResidualBlock(curr_channels, curr_channels)\n",
    "        \n",
    "        # end\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, num_channels=curr_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(curr_channels, 2*z_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(2*z_channels, 2*z_channels, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_in(x)\n",
    "        \n",
    "        for down in self.down:\n",
    "            x = down.block(x)\n",
    "            x = down.downsample(x)\n",
    "\n",
    "        x = self.mid.res_block_1(x)\n",
    "        x = self.mid.attn_block_1(x)\n",
    "        x = self.mid.res_block_2(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, ch_mult: List[int]=[1, 2, 4, 8], dropout: float=0.0, z_channels: int=8):\n",
    "        super().__init__()\n",
    "\n",
    "        ch = 128\n",
    "        block_in = ch*ch_mult[-1]\n",
    "        self.conv_in = nn.Sequential(nn.Conv2d(z_channels, z_channels, kernel_size=1, padding=0), \n",
    "                                     nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "    \n",
    "        # mid\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.res_block_1 = ResidualBlock(block_in, block_in)\n",
    "        self.mid.attn_block_1 = AttentionBlock(in_channels=block_in)\n",
    "        self.mid.res_block_2 = ResidualBlock(block_in, block_in)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i in reversed(range(len(ch_mult))):\n",
    "            block_out = ch * ch_mult[i]\n",
    "            block = nn.Sequential(\n",
    "                ResidualBlock(block_in, block_out),\n",
    "                ResidualBlock(block_out, block_out),\n",
    "                ResidualBlock(block_out, block_out)\n",
    "            )\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            if i != 0:\n",
    "                up.upsample = UpSample(in_channels=block_out)\n",
    "            else:\n",
    "                up.upsample = nn.Identity()\n",
    "            self.up.append(up)\n",
    "            block_in = block_out\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, num_channels=ch), \n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, 3, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        \n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, channels, h, w)\n",
    "        x /= 0.18215\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        for up in self.up:\n",
    "            x = up.block(x)\n",
    "            x = up.upsample(x)\n",
    "\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels: int=3, z_channels: int=8):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder(in_channels=in_channels)\n",
    "        self.decoder = VAE_Decoder(z_channels=z_channels)\n",
    "\n",
    "    def encode(self, x: torch.Tensor, noise=None) -> torch.Tensor:\n",
    "        # z: (n, c, h, w)\n",
    "        z = self.encoder(x)\n",
    "        mean, log_variance = z.chunk(2, dim=1)\n",
    "        log_variance = torch.clamp(log_variance, -20, 30)\n",
    "        variance = log_variance.exp()\n",
    "        stdev = torch.sqrt(variance)\n",
    "        if noise:\n",
    "            return mean + stdev * noise\n",
    "        else:\n",
    "            return mean + stdev * torch.randn_like(stdev)\n",
    "        \n",
    "\n",
    "    def decode(self, z: torch.Tensor):\n",
    "        return self.decoder(z)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a51d46-3261-4f51-b0db-0d2fbd7ea2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/cond_encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/cond_encoder.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from .attention import MultiheadSelfAttention\n",
    "from .activation_fn import QuickGELU\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int=49408, embed_dim: int=768, max_len: int=77):\n",
    "        super().__init__()\n",
    "        self.text_embedding = TextEmbedding(n_vocab=n_vocab, embed_dim=embed_dim, max_len=max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(num_heads=12, embed_dim=embed_dim, ffn_dim=embed_dim*8) for _ in range(12)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## TODO: Padding text before putting into embedding\n",
    "        \n",
    "        x = self.text_embedding(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "        \n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, embed_dim: int, max_len: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, embed_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x += self.positional_encoding\n",
    "        return x\n",
    "        \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads: int, embed_dim: int, ffn_dim: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "        self.attn_1 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embed_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            QuickGELU(),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout_2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        x = x.type(torch.long)\n",
    "        skip_connection = x\n",
    "        x = self.attn_1(x=x.type(torch.float), lookahead_mask=True)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layernorm_1(x + skip_connection)\n",
    "\n",
    "        skip_connection = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout_2(x)\n",
    "        output = self.layernorm_2(x + skip_connection)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4b982a-240b-4f2e-a8da-c71d65e9dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/attention.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/attention.py\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: int=None, use_bias=True):\n",
    "        super().__init__()\n",
    "        if not cond_dim:\n",
    "            cond_dim = embedding_dim\n",
    "        self.proj_q = nn.Linear(embedding_dim, embedding_dim, bias=use_bias)\n",
    "        self.proj_k = nn.Linear(cond_dim, embedding_dim, bias=use_bias)\n",
    "        self.proj_v = nn.Linear(cond_dim, embedding_dim, bias=use_bias)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // self.num_heads\n",
    "        self.proj_out = nn.Linear(embedding_dim, embedding_dim, bias=use_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor=None, lookahead_mask: bool=True) -> torch.Tensor:\n",
    "        # x: (n, seq_len, embedding_dim)\n",
    "        \n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "\n",
    "        q = self.proj_q(x)\n",
    "        if cond is None:\n",
    "            cond = x\n",
    "            k = self.proj_k(cond).unsqueeze(1)\n",
    "            v = self.proj_v(cond).unsqueeze(1)\n",
    "        else:\n",
    "            k = self.proj_k(cond)\n",
    "            v = self.proj_v(cond)\n",
    "            \n",
    "        print(k.shape, v.shape)\n",
    "        # (batch_size, seq_len, embedding_dim) -> (n, seq_len, num_heads, head_dim) -> (n, num_heads, seq_len, head_dim)\n",
    "        q = q.view(*q.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "       \n",
    "\n",
    "        # (n, num_heads, seq_len, head_dim) @ (n, num_heads, head_dim, seq_len) -> (n, seq_len, seq_len, seq_len)\n",
    "        attn_weights = q @ k.transpose(-1, -2)\n",
    "        if lookahead_mask:\n",
    "            mask = torch.ones_like(attn_weights, dtype=torch.bool).triu(1)\n",
    "            attn_weights.masked_fill_(mask, -torch.inf)\n",
    "            \n",
    "        attn_weights /= math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # (n, num_heads, seq_len, seq_len) @ (n, num_heads, seq_len, head_dim) -> (n, num_heads, seq_len, head_dim)\n",
    "        attn_weights = attn_weights @ v\n",
    "\n",
    "        # (n, num_heads, seq_len, head_dim) -> (n, seq_len, num_heads, head_dim) -> (n, seq_len, embedding_dim)\n",
    "        attn_weights = attn_weights.transpose(1, 2).reshape((batch_size, seq_len, embedding_dim))\n",
    "\n",
    "        out = self.proj_out(attn_weights)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd30d3d4-a752-4568-9cd7-e25d251fecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/unet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/unet.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from .attention import MultiheadSelfAttention\n",
    "from .activation_fn import GeGELU\n",
    "from typing import Optional, List\n",
    "\n",
    "class UNet_TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: Optional[int]):\n",
    "        super().__init__()\n",
    "        channels = embedding_dim * num_heads\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "        self.transformer_block = UNet_AttentionBlock(num_heads=num_heads, embedding_dim=channels, cond_dim=cond_dim)\n",
    "\n",
    "        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (b, c, h, w)\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x_in = x\n",
    "\n",
    "        x = self.groupnorm(x)\n",
    "        x = self.conv_input(x)\n",
    "\n",
    "        # (b, c, h, w) -> (b, c, h * w) -> (b, h * w, c)\n",
    "        x = x.view(b, c, -1).transpose(-1, -2)\n",
    "\n",
    "        x = self.transformer_block(x=x, cond=cond)\n",
    "\n",
    "        x = x.transpose(-1, -2).view(b, c, h, w)\n",
    "\n",
    "        x = self.conv_output(x)\n",
    "\n",
    "        return x + x_in\n",
    "        \n",
    "class UNet_AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: Optional[int]):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embedding_dim % num_heads:\n",
    "            raise ValueError('Number of heads must be divisible by Embedding Dimension')\n",
    "            \n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn1 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embedding_dim, cond_dim=cond_dim, use_bias=False)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn2 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embedding_dim, cond_dim=cond_dim, use_bias=False)\n",
    "\n",
    "        self.layer_norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            GeGELU(embedding_dim, embedding_dim * 4),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.attn1(self.layer_norm1(x), cond=cond) + x\n",
    "\n",
    "        x = self.attn2(self.layer_norm2(x), cond=cond) + x\n",
    "\n",
    "        x = self.ff(self.layer_norm3(x)) + x\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class UNet_ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, t_embed_dim: int):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.groupnorm_1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "            self.conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "            self.groupnorm_2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n",
    "            self.conv_2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            self.t_embed = nn.Linear(t_embed_dim, out_channels)\n",
    "            \n",
    "            if in_channels == out_channels:\n",
    "                self.proj_input = nn.Identity()\n",
    "            else:\n",
    "                self.proj_input = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (n, c, h, w)\n",
    "        h = self.groupnorm_1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_1(h)\n",
    "\n",
    "        # time: (1, t_embed_dim) -> (1, out_channels)\n",
    "        time = F.silu(t_embed)\n",
    "        time = self.t_embed(t_embed)\n",
    "\n",
    "        # (n, out_channels, h, w) + (1, out_channels, 1, 1) -> (n, out_channels, h, w)\n",
    "        h = h + time[:, :, None, None]\n",
    "\n",
    "        h = self.groupnorm_2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_2(h)\n",
    "        return h + self.proj_input(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, t_embed_dim):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            # (1, 320) -> (1, 1280)\n",
    "            nn.Linear(t_embed_dim, t_embed_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            # (1, 1280) -> (1, 1280)\n",
    "            nn.Linear(t_embed_dim * 4,  t_embed_dim * 4))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.ffn(x)\n",
    "\n",
    "class TimeStepSequential(nn.Sequential):\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, cond=None) -> torch.Tensor:\n",
    "        for layer in self:\n",
    "            if isinstance(layer, UNet_ResBlock):\n",
    "                x = layer(x, t_embed)\n",
    "            elif isinstance(layer, UNet_TransformerEncoder):\n",
    "                x = layer(x, cond)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "        \n",
    "class UNet_Downsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet_Upsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(self.upsample(x))\n",
    "    \n",
    "class UNet_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int=8, num_heads: int=8, t_embed_dim: int=1280, cond_dim: int=768, ch_multiplier=[1, 2, 4, 4]):\n",
    "        super().__init__()\n",
    "        ch = 320\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.down = nn.ModuleList()\n",
    "        in_ch_multiplier = [1] + ch_multiplier\n",
    "        \n",
    "        for i in range(len(ch_multiplier)):\n",
    "            down = nn.Module()\n",
    "            in_channels = ch * in_ch_multiplier[i]\n",
    "            out_channels = ch * ch_multiplier[i]\n",
    "            block = TimeStepSequential(\n",
    "                UNet_ResBlock(in_channels, out_channels, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_channels // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_channels, out_channels, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_channels // num_heads, cond_dim=cond_dim)\n",
    "            )\n",
    "            if i != len(ch_multiplier) - 1:\n",
    "                downsample = UNet_Downsample(out_channels)\n",
    "            else:\n",
    "                downsample = nn.Identity()\n",
    "            \n",
    "            down.block = block\n",
    "            down.downsample = downsample\n",
    "            \n",
    "            self.down.append(down)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, cond: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv_in(x)\n",
    "        skip_connections = [x]\n",
    "        for down in self.down:\n",
    "            x = down.block(x, t_embed, cond)\n",
    "            skip_connections.append(x)\n",
    "            x = down.downsample(x)\n",
    "            \n",
    "        return x, skip_connections\n",
    "\n",
    "class UNet_Decoder(nn.Module):\n",
    "    def __init__(self, num_heads: int=8, t_embed_dim: int=1280, cond_dim: int=768, ch_multiplier=[1, 2, 4, 4]):\n",
    "        super().__init__()\n",
    "        ch = 320\n",
    "        in_ch_multiplier = [1] + ch_multiplier\n",
    "        \n",
    "        self.up = nn.ModuleList()\n",
    "        for i in reversed(range(4)):\n",
    "            up = nn.Module()\n",
    "            in_ch = in_ch_multiplier[i+1] * ch\n",
    "            out_ch = in_ch_multiplier[i] * ch\n",
    "            block = TimeStepSequential(\n",
    "                UNet_ResBlock(in_ch * 2, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_ch, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_ch, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim)\n",
    "            )\n",
    "            \n",
    "            if i != 0:\n",
    "                upsample = UNet_Upsample(out_ch)\n",
    "            else:\n",
    "                upsample = nn.Identity()\n",
    "\n",
    "            up.block = block\n",
    "            up.upsample = upsample\n",
    "\n",
    "            self.up.append(up)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip_connections: List[torch.Tensor], t_embed: torch.Tensor, cond: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        # x: (b, c, h, w)\n",
    "        for up in self.up:\n",
    "            x = torch.cat([x, skip_connections.pop()], dim=1)\n",
    "            x = up.block(x, t_embed, cond)\n",
    "            x = up.upsample(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int=8, out_channels: int=8, num_heads: int=8, t_embed_dim: int=320, cond_dim: int=768):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(t_embed_dim)\n",
    "        self.encoder = UNet_Encoder(in_channels=in_channels, num_heads=num_heads, t_embed_dim=t_embed_dim * 4, cond_dim=cond_dim)\n",
    "        self.bottle_neck = TimeStepSequential(\n",
    "            UNet_ResBlock(1280, 1280, t_embed_dim * 4),\n",
    "            UNet_TransformerEncoder(num_heads=8, embedding_dim=160, cond_dim=cond_dim),\n",
    "            UNet_ResBlock(1280, 1280, t_embed_dim * 4)\n",
    "        )\n",
    "        self.decoder = UNet_Decoder(num_heads=num_heads, t_embed_dim=t_embed_dim * 4, cond_dim=cond_dim)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.GroupNorm(32, 320),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(320, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # (1, 320) -> (1, 1280)\n",
    "        t_embed = self.time_embedding(t_embed)\n",
    "        \n",
    "        x, skip_connections = self.encoder(x, t_embed, cond)\n",
    "        x = self.bottle_neck(x, t_embed, cond)\n",
    "        print('decoder: ')\n",
    "        x = self.decoder(x, skip_connections, t_embed, cond)\n",
    "        \n",
    "        output = self.output(x)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19a19d99-54fe-4387-9485-2646148316d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 1024]) torch.Size([1, 1, 64, 1024])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 1, 1024]' is invalid for input of size 65536",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m cond \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m))\n\u001b[1;32m      7\u001b[0m encoder \u001b[38;5;241m=\u001b[39m VAE()\n\u001b[0;32m----> 8\u001b[0m out_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m out_tensor\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/vae.py:169\u001b[0m, in \u001b[0;36mVAE.encode\u001b[0;34m(self, x, noise)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# z: (n, c, h, w)\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     mean, log_variance \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    171\u001b[0m     log_variance \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(log_variance, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/vae.py:100\u001b[0m, in \u001b[0;36mVAE_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m     x \u001b[38;5;241m=\u001b[39m down\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mres_block_1(x)\n\u001b[0;32m--> 100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_block_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mres_block_2(x)\n\u001b[1;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(x)\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/vae.py:45\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m x_norm \u001b[38;5;241m=\u001b[39m x_norm\u001b[38;5;241m.\u001b[39mview((batch_size, channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# (n, h * w, c)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# (n, h * w, c) -> (n, c, h * w) -> (n, c, h, w)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Study/pytorch-course/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/attention.py:36\u001b[0m, in \u001b[0;36mMultiheadSelfAttention.forward\u001b[0;34m(self, x, cond, lookahead_mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# (batch_size, seq_len, embedding_dim) -> (n, seq_len, num_heads, head_dim) -> (n, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mq\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     37\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mv\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# (n, num_heads, seq_len, head_dim) @ (n, num_heads, head_dim, seq_len) -> (n, seq_len, seq_len, seq_len)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 1, 1024]' is invalid for input of size 65536"
     ]
    }
   ],
   "source": [
    "from models.vae import VAE\n",
    "import torch \n",
    "\n",
    "random_tensor = torch.ones((1, 3, 64, 64), dtype=torch.long)\n",
    "time_tensor = torch.randn((1, 320))\n",
    "cond = torch.randn((1, 768))\n",
    "encoder = VAE()\n",
    "out_tensor = encoder.encode(random_tensor.type(torch.float))\n",
    "out_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f31d0-1c78-4ec1-bb74-22c378a06f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
