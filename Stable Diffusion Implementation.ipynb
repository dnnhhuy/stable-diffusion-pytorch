{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0a9344-de43-4e97-b928-bce1b4ef6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/resnet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/resnet.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.groupnorm_1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.groupnorm_2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.proj_input = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        else:\n",
    "            self.proj_input = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_norm = self.groupnorm_1(x)\n",
    "        out = F.silu(x_norm)\n",
    "        out = self.conv_1(out)\n",
    "\n",
    "        out = self.groupnorm_2(out)\n",
    "        out = F.silu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv_2(out)\n",
    "        \n",
    "        return out + self.proj_input(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff97a0d-cb1a-483d-904f-1b856fe6f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/vae.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/vae.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from .resnet import ResidualBlock\n",
    "from .attention import MultiheadSelfAttention\n",
    "from typing import List\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.pad = (0, 1, 0, 1)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.pad(x, self.pad)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.upsample(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "        self.attn = MultiheadSelfAttention(num_heads=1, embedding_dim=in_channels)\n",
    "\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (n, c, h, w)\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        x_norm = self.groupnorm(x)\n",
    "        # (n, c, h, w) -> (n, c, h * w) -> (n, h * w, c)\n",
    "        x_norm = x_norm.view((batch_size, channels, -1)).transpose(1, 2)\n",
    "\n",
    "        # (n, h * w, c)\n",
    "        out = self.attn(x=x_norm)\n",
    "\n",
    "        # (n, h * w, c) -> (n, c, h * w) -> (n, c, h, w)\n",
    "        out = out.transpose(1, 2).reshape(x.shape)\n",
    "        \n",
    "        return out + x\n",
    "        \n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int, ch_mult: List[int]=[1, 2, 4, 8], dropout: float=0.0, z_channels: int=8):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, 128, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # start\n",
    "        self.down = nn.ModuleList()\n",
    "        in_ch_mult = [1] + ch_mult\n",
    "        ch = 128\n",
    "        for i in range(len(ch_mult)):\n",
    "            block_in = ch * in_ch_mult[i]\n",
    "            block_out = ch * ch_mult[i]\n",
    "            block = nn.Sequential(\n",
    "                ResidualBlock(block_in, block_out, dropout),\n",
    "                ResidualBlock(block_out, block_out, dropout),\n",
    "            )\n",
    "            \n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            if i != len(ch_mult) - 1:\n",
    "                down.downsample = Downsample(block_out)\n",
    "            else:\n",
    "                down.downsample = nn.Identity()\n",
    "                \n",
    "            self.down.append(down)\n",
    "            curr_channels = block_out\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.res_block_1 = ResidualBlock(curr_channels, curr_channels)\n",
    "        self.mid.attn_block_1 = AttentionBlock(in_channels=curr_channels)\n",
    "        self.mid.res_block_2 = ResidualBlock(curr_channels, curr_channels)\n",
    "        \n",
    "        # end\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, num_channels=curr_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(curr_channels, 2*z_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(2*z_channels, 2*z_channels, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_in(x)\n",
    "        \n",
    "        for down in self.down:\n",
    "            x = down.block(x)\n",
    "            x = down.downsample(x)\n",
    "\n",
    "        x = self.mid.res_block_1(x)\n",
    "        x = self.mid.attn_block_1(x)\n",
    "        x = self.mid.res_block_2(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, ch_mult: List[int]=[1, 2, 4, 8], dropout: float=0.0, z_channels: int=8):\n",
    "        super().__init__()\n",
    "\n",
    "        ch = 128\n",
    "        block_in = ch*ch_mult[-1]\n",
    "        self.conv_in = nn.Sequential(nn.Conv2d(z_channels, z_channels, kernel_size=1, padding=0), \n",
    "                                     nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "    \n",
    "        # mid\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.res_block_1 = ResidualBlock(block_in, block_in)\n",
    "        self.mid.attn_block_1 = AttentionBlock(in_channels=block_in)\n",
    "        self.mid.res_block_2 = ResidualBlock(block_in, block_in)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i in reversed(range(len(ch_mult))):\n",
    "            block_out = ch * ch_mult[i]\n",
    "            block = nn.Sequential(\n",
    "                ResidualBlock(block_in, block_out),\n",
    "                ResidualBlock(block_out, block_out),\n",
    "                ResidualBlock(block_out, block_out)\n",
    "            )\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            if i != 0:\n",
    "                up.upsample = UpSample(in_channels=block_out)\n",
    "            else:\n",
    "                up.upsample = nn.Identity()\n",
    "            self.up.append(up)\n",
    "            block_in = block_out\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=32, num_channels=ch), \n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(ch, 3, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        \n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (batch_size, channels, h, w)\n",
    "        x /= 0.18215\n",
    "        x = self.conv_in(x)\n",
    "\n",
    "        for up in self.up:\n",
    "            x = up.block(x)\n",
    "            x = up.upsample(x)\n",
    "\n",
    "        out = self.out(x)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels: int=3, z_channels: int=8):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder(in_channels=in_channels)\n",
    "        self.decoder = VAE_Decoder(z_channels=z_channels)\n",
    "\n",
    "    def encode(self, x: torch.Tensor, noise=None) -> torch.Tensor:\n",
    "        # z: (n, c, h, w)\n",
    "        z = self.encoder(x)\n",
    "        mean, log_variance = z.chunk(2, dim=1)\n",
    "        log_variance = torch.clamp(log_variance, -20, 30)\n",
    "        variance = log_variance.exp()\n",
    "        stdev = torch.sqrt(variance)\n",
    "        if noise:\n",
    "            return mean + stdev * noise\n",
    "        else:\n",
    "            return mean + stdev * torch.randn_like(stdev)\n",
    "        \n",
    "\n",
    "    def decode(self, z: torch.Tensor):\n",
    "        return self.decoder(z)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a51d46-3261-4f51-b0db-0d2fbd7ea2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/cond_encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/cond_encoder.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from .attention import MultiheadSelfAttention\n",
    "from .activation_fn import QuickGELU\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, n_vocab: int=49408, embed_dim: int=768, max_len: int=77):\n",
    "        super().__init__()\n",
    "        self.text_embedding = TextEmbedding(n_vocab=n_vocab, embed_dim=embed_dim, max_len=max_len)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoder(num_heads=12, embed_dim=embed_dim, ffn_dim=embed_dim*8) for _ in range(12)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ## TODO: Padding text before putting into embedding\n",
    "        \n",
    "        x = self.text_embedding(x)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "        \n",
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, embed_dim: int, max_len: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(max_len, embed_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x += self.positional_encoding\n",
    "        return x\n",
    "        \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads: int, embed_dim: int, ffn_dim: int, dropout: float=0.0):\n",
    "        super().__init__()\n",
    "        self.attn_1 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embed_dim)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_dim),\n",
    "            QuickGELU(),\n",
    "            nn.Linear(ffn_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout_2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        x = x.type(torch.long)\n",
    "        skip_connection = x\n",
    "        x = self.attn_1(x=x.type(torch.float), lookahead_mask=True)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.layernorm_1(x + skip_connection)\n",
    "\n",
    "        skip_connection = x\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout_2(x)\n",
    "        output = self.layernorm_2(x + skip_connection)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4b982a-240b-4f2e-a8da-c71d65e9dd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/attention.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/attention.py\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: int=768, use_bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if not cond_dim:\n",
    "            cond_dim = embedding_dim\n",
    "            \n",
    "        self.proj_q = nn.Linear(embedding_dim, embedding_dim, bias=use_bias)\n",
    "        self.proj_k = nn.Linear(cond_dim, embedding_dim, bias=use_bias)\n",
    "        self.proj_v = nn.Linear(cond_dim, embedding_dim, bias=use_bias)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_dim // self.num_heads\n",
    "        self.proj_out = nn.Linear(embedding_dim, embedding_dim, bias=use_bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor=None, lookahead_mask: bool=True) -> torch.Tensor:\n",
    "        # x: (n, seq_len, embedding_dim)\n",
    "        # cond: (n, seq_len, dim)\n",
    "        \n",
    "        batch_size, seq_len, embedding_dim = x.shape\n",
    "       \n",
    "        if cond is None:\n",
    "            cond = x\n",
    "            \n",
    "        q = self.proj_q(x)\n",
    "        k = self.proj_k(cond)\n",
    "        v = self.proj_v(cond)\n",
    "            \n",
    "        # (batch_size, seq_len, embedding_dim) -> (n, seq_len, num_heads, head_dim) -> (n, num_heads, seq_len, head_dim)\n",
    "        q = q.view(*q.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(*k.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(*v.shape[:2], self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "       \n",
    "\n",
    "        # (n, num_heads, seq_len, head_dim) @ (n, num_heads, head_dim, seq_len) -> (n, seq_len, seq_len, seq_len)\n",
    "        attn_weights = q @ k.transpose(-1, -2)\n",
    "        if lookahead_mask:\n",
    "            mask = torch.ones_like(attn_weights, dtype=torch.bool).triu(1)\n",
    "            attn_weights.masked_fill_(mask, -torch.inf)\n",
    "            \n",
    "        attn_weights /= math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # (n, num_heads, seq_len, seq_len) @ (n, num_heads, seq_len, head_dim) -> (n, num_heads, seq_len, head_dim)\n",
    "        attn_weights = attn_weights @ v\n",
    "\n",
    "        # (n, num_heads, seq_len, head_dim) -> (n, seq_len, num_heads, head_dim) -> (n, seq_len, embedding_dim)\n",
    "        attn_weights = attn_weights.transpose(1, 2).reshape((batch_size, seq_len, embedding_dim))\n",
    "\n",
    "        out = self.proj_out(attn_weights)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd30d3d4-a752-4568-9cd7-e25d251fecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/unet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/unet.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from .attention import MultiheadSelfAttention\n",
    "from .activation_fn import GeGELU\n",
    "from typing import Optional, List\n",
    "\n",
    "class UNet_TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: int=768):\n",
    "        super().__init__()\n",
    "        channels = embedding_dim * num_heads\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "        self.transformer_block = UNet_AttentionBlock(num_heads=num_heads, embedding_dim=channels, cond_dim=cond_dim)\n",
    "\n",
    "        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor=None) -> torch.Tensor:\n",
    "        # x: (b, c, h, w)\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x_in = x\n",
    "\n",
    "        x = self.groupnorm(x)\n",
    "        x = self.conv_input(x)\n",
    "\n",
    "        # (b, c, h, w) -> (b, c, h * w) -> (b, h * w, c)\n",
    "        x = x.view(b, c, -1).transpose(-1, -2)\n",
    "\n",
    "        x = self.transformer_block(x=x, cond=cond)\n",
    "\n",
    "        x = x.transpose(-1, -2).view(b, c, h, w)\n",
    "\n",
    "        x = self.conv_output(x)\n",
    "\n",
    "        return x + x_in\n",
    "        \n",
    "class UNet_AttentionBlock(nn.Module):\n",
    "    def __init__(self, num_heads: int, embedding_dim: int, cond_dim: int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        if embedding_dim % num_heads:\n",
    "            raise ValueError('Number of heads must be divisible by Embedding Dimension')\n",
    "            \n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn1 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embedding_dim, cond_dim=cond_dim, use_bias=False)\n",
    "        \n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "        self.attn2 = MultiheadSelfAttention(num_heads=num_heads, embedding_dim=embedding_dim, cond_dim=cond_dim, use_bias=False)\n",
    "\n",
    "        self.layer_norm3 = nn.LayerNorm(embedding_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            GeGELU(embedding_dim, embedding_dim * 4),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim))\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.attn1(self.layer_norm1(x), cond=cond) + x\n",
    "\n",
    "        x = self.attn2(self.layer_norm2(x), cond=cond) + x\n",
    "\n",
    "        x = self.ff(self.layer_norm3(x)) + x\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class UNet_ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, t_embed_dim: int):\n",
    "            super().__init__()\n",
    "            \n",
    "            self.groupnorm_1 = nn.GroupNorm(num_groups=32, num_channels=in_channels)\n",
    "            self.conv_1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "            self.groupnorm_2 = nn.GroupNorm(num_groups=32, num_channels=out_channels)\n",
    "            self.conv_2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            self.t_embed = nn.Linear(t_embed_dim, out_channels)\n",
    "            \n",
    "            if in_channels == out_channels:\n",
    "                self.proj_input = nn.Identity()\n",
    "            else:\n",
    "                self.proj_input = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (n, c, h, w)\n",
    "        h = self.groupnorm_1(x)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_1(h)\n",
    "\n",
    "        # time: (1, t_embed_dim) -> (1, out_channels)\n",
    "        time = F.silu(t_embed)\n",
    "        time = self.t_embed(t_embed)\n",
    "\n",
    "        # (n, out_channels, h, w) + (1, out_channels, 1, 1) -> (n, out_channels, h, w)\n",
    "        h = h + time[:, :, None, None]\n",
    "\n",
    "        h = self.groupnorm_2(h)\n",
    "        h = F.silu(h)\n",
    "        h = self.conv_2(h)\n",
    "        return h + self.proj_input(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, t_embed_dim: int=320):\n",
    "        super().__init__()\n",
    "        self.t_embed_dim = t_embed_dim\n",
    "        self.ffn = nn.Sequential(\n",
    "            # (1, 320) -> (1, 1280)\n",
    "            nn.Linear(t_embed_dim, t_embed_dim * 4),\n",
    "            nn.SiLU(),\n",
    "            # (1, 1280) -> (1, 1280)\n",
    "            nn.Linear(t_embed_dim * 4,  t_embed_dim * 4))\n",
    "\n",
    "    def _get_time_embedding(self, timestep):\n",
    "        half = self.t_embed_dim // 2\n",
    "        freqs = torch.pow(1000, -torch.arange(0, half, dtype=torch.float32)/half)\n",
    "        x = torch.tensor([timestep], dtype=torch.float32, device=timestep.device)[None, :] * freqs[None, :].to(timestep.device)\n",
    "        return torch.cat([torch.cos(x), torch.sin(x)], dim=1)\n",
    "            \n",
    "    def forward(self, timestep: int) -> torch.Tensor:\n",
    "        t_embed = self._get_time_embedding(timestep)\n",
    "        print(t_embed.device)\n",
    "        return self.ffn(t_embed)\n",
    "\n",
    "class TimeStepSequential(nn.Sequential):\n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, cond=None) -> torch.Tensor:\n",
    "        for layer in self:\n",
    "            if isinstance(layer, UNet_ResBlock):\n",
    "                x = layer(x, t_embed)\n",
    "            elif isinstance(layer, UNet_TransformerEncoder):\n",
    "                x = layer(x, cond)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "        \n",
    "class UNet_Downsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet_Upsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(self.upsample(x))\n",
    "    \n",
    "class UNet_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels: int=8, num_heads: int=8, t_embed_dim: int=1280, cond_dim: int=768, ch_multiplier=[1, 2, 4, 4]):\n",
    "        super().__init__()\n",
    "        ch = 320\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.down = nn.ModuleList()\n",
    "        in_ch_multiplier = [1] + ch_multiplier\n",
    "        \n",
    "        for i in range(len(ch_multiplier)):\n",
    "            down = nn.Module()\n",
    "            in_channels = ch * in_ch_multiplier[i]\n",
    "            out_channels = ch * ch_multiplier[i]\n",
    "            block = TimeStepSequential(\n",
    "                UNet_ResBlock(in_channels, out_channels, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_channels // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_channels, out_channels, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_channels // num_heads, cond_dim=cond_dim)\n",
    "            )\n",
    "            if i != len(ch_multiplier) - 1:\n",
    "                downsample = UNet_Downsample(out_channels)\n",
    "            else:\n",
    "                downsample = nn.Identity()\n",
    "            \n",
    "            down.block = block\n",
    "            down.downsample = downsample\n",
    "            \n",
    "            self.down.append(down)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor, t_embed: torch.Tensor, cond: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        \n",
    "        x = self.conv_in(x)\n",
    "        skip_connections = [x]\n",
    "        for down in self.down:\n",
    "            x = down.block(x, t_embed, cond)\n",
    "            skip_connections.append(x)\n",
    "            x = down.downsample(x)\n",
    "            \n",
    "        return x, skip_connections\n",
    "\n",
    "class UNet_Decoder(nn.Module):\n",
    "    def __init__(self, num_heads: int=8, t_embed_dim: int=1280, cond_dim: int=768, ch_multiplier=[1, 2, 4, 4]):\n",
    "        super().__init__()\n",
    "        ch = 320\n",
    "        in_ch_multiplier = [1] + ch_multiplier\n",
    "        \n",
    "        self.up = nn.ModuleList()\n",
    "        for i in reversed(range(4)):\n",
    "            up = nn.Module()\n",
    "            in_ch = in_ch_multiplier[i+1] * ch\n",
    "            out_ch = in_ch_multiplier[i] * ch\n",
    "            block = TimeStepSequential(\n",
    "                UNet_ResBlock(in_ch * 2, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_ch, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim),\n",
    "                UNet_ResBlock(out_ch, out_ch, t_embed_dim), \n",
    "                UNet_TransformerEncoder(num_heads=num_heads, embedding_dim=out_ch // num_heads, cond_dim=cond_dim)\n",
    "            )\n",
    "            \n",
    "            if i != 0:\n",
    "                upsample = UNet_Upsample(out_ch)\n",
    "            else:\n",
    "                upsample = nn.Identity()\n",
    "\n",
    "            up.block = block\n",
    "            up.upsample = upsample\n",
    "\n",
    "            self.up.append(up)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip_connections: List[torch.Tensor], t_embed: torch.Tensor, cond: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        # x: (b, c, h, w)\n",
    "        for up in self.up:\n",
    "            x = torch.cat([x, skip_connections.pop()], dim=1)\n",
    "            x = up.block(x, t_embed, cond)\n",
    "            x = up.upsample(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels: int=8, out_channels: int=8, num_heads: int=8, t_embed_dim: int=320, cond_dim: int=768):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(t_embed_dim)\n",
    "        self.encoder = UNet_Encoder(in_channels=in_channels, num_heads=num_heads, t_embed_dim=t_embed_dim * 4, cond_dim=cond_dim)\n",
    "        self.bottle_neck = TimeStepSequential(\n",
    "            UNet_ResBlock(1280, 1280, t_embed_dim * 4),\n",
    "            UNet_TransformerEncoder(num_heads=8, embedding_dim=160, cond_dim=cond_dim),\n",
    "            UNet_ResBlock(1280, 1280, t_embed_dim * 4)\n",
    "        )\n",
    "        self.decoder = UNet_Decoder(num_heads=num_heads, t_embed_dim=t_embed_dim * 4, cond_dim=cond_dim)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.GroupNorm(32, 320),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(320, out_channels, kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, timestep: int, cond: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # t: int -> (1, 1280)\n",
    "        t_embed = self.time_embedding(timestep)\n",
    "        \n",
    "        x, skip_connections = self.encoder(x, t_embed, cond)\n",
    "        x = self.bottle_neck(x, t_embed, cond)\n",
    "        x = self.decoder(x, skip_connections, t_embed, cond)\n",
    "        \n",
    "        output = self.output(x)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0667bae9-5049-45de-a377-d97170df6c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/diffusion.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/diffusion.py\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from .vae import VAE\n",
    "from .unet import UNet\n",
    "from .cond_encoder import TextEncoder\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from .utils import denormalize_img\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "Z_HEIGHT = 64 // 8\n",
    "Z_WIDTH = 64 // 8\n",
    "\n",
    "class StableDiffusion:\n",
    "    def __init__(self, noise_step: int=1000, beta_start: float=1e-4, beta_end: float=0.02):\n",
    "        self.betas = torch.linspace(beta_start, beta_end, noise_step, dtype=torch.float)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_hat = torch.cumprod(self.alphas, dim=0)\n",
    "        self.noise_step = noise_step\n",
    "\n",
    "        self.timesteps = torch.from_numpy(np.arange(0, noise_step)[::-1].copy())\n",
    "        self.vae = VAE()\n",
    "        self.unet = UNet()\n",
    "        self.cond_encoder = TextEncoder()\n",
    "\n",
    "    def _set_inference_step(self, inference_steps=50):\n",
    "        self.inference_steps = inference_steps\n",
    "        ratio = self.noise_step // self.inference_steps\n",
    "        self.timesteps = torch.from_numpy((np.arange(0, self.inference_steps) * ratio).round()[::-1].copy().astype(np.int64))\n",
    "        \n",
    "\n",
    "    def _get_prev_timestep(self, timestep: int):\n",
    "        prev_t = timestep - self.noise_step // self.inference_steps\n",
    "        return prev_t\n",
    "\n",
    "    def set_strength(self, strength: float=0.8):\n",
    "        start_t = self.inference_steps - (self.inference_steps * strength)\n",
    "        self.timesteps = self.timesteps[start_t:]\n",
    "\n",
    "    # x_t ~ q(x_t | x_0) = N(x_t, sqrt(a_hat_t) * x_0, sqrt(1 - a_hat_t) * I)\n",
    "    def forward_process(self, x_0: torch.Tensor, timestep: int):\n",
    "        # x_0: (b, c, h, w)\n",
    "        t = timestep\n",
    "        # (1,) -> (1, 1, 1, 1)\n",
    "        alpha_hat_t = self.alphas_hat[t, None, None, None].to(x_0.device)\n",
    "              \n",
    "        noise = torch.randn_like(x_0, dtype=torch.float32, device=x_0.device)\n",
    "        latent = torch.sqrt(alpha_hat_t) * x_0 + torch.sqrt(1 - alpha_hat_t) * noise\n",
    "        \n",
    "        return latent, noise\n",
    "\n",
    "    # x_(t-1) ~ p(x_(t-1) | x_t) = N(x_(t - 1), mu_theta(x_t, x_0), beta_tilda_t * I)\n",
    "    # mu_theta(x_t, x-0)1/sqrt(alpha_hat_t) * (x_t - (1 - alpha_t)/sqrt(1 - alpha_hat_t) * epsilon_t)\n",
    "    # beta_tilda_t = (1 - alpha_hat_(t-1)) / (1 - alpha_t) * beta_t\n",
    "    def reverse_process(self, x_t: torch.Tensor, timestep: int, model_output=torch.Tensor) -> torch.Tensor:\n",
    "        t = timestep\n",
    "        prev_t = self._get_prev_timestep(t)\n",
    "\n",
    "        \n",
    "        alpha_t = self.alphas[t, None, None, None].to(x_t.device)\n",
    "        alpha_hat_t = self.alphas_hat[t, None, None, None].to(x_t.device)\n",
    "        prev_alpha_hat_t = self.alphas_hat[prev_t] if prev_t >= 0 else torch.tensor(1.0)\n",
    "        prev_alpha_hat_t = prev_alpha_hat_t.to(x_t.device)\n",
    "        \n",
    "        mu = 1/torch.sqrt(alpha_t) * (x_t - (1 - alpha_t)/torch.sqrt(1 - alpha_hat_t) * model_output)\n",
    "\n",
    "        stdev = 0\n",
    "        if t > 0:\n",
    "            variance = (1 - prev_alpha_hat_t) / (1 - alpha_hat_t) * self.betas[t]\n",
    "            variance = torch.clamp(variance, min=1e-20)\n",
    "            stdev = torch.sqrt(variance)\n",
    "            \n",
    "        noise = torch.randn_like(x_t, dtype=torch.float32, device=x_t.device)\n",
    "        less_noise_sample = mu + stdev * noise\n",
    "        return less_noise_sample\n",
    "    \n",
    "    def generate(self, input_image: Image, \n",
    "                 transforms: torchvision.transforms,\n",
    "                 prompt: str,\n",
    "                 uncond_promt: str,\n",
    "                 do_cfg: bool,\n",
    "                 cfg_scale: int,\n",
    "                 device: torch.device,\n",
    "                 strength:float,\n",
    "                 inference_steps: int,\n",
    "                 tokenizer=None) -> torch.Tensor:\n",
    "        \n",
    "        z_shape = (1, 8, Z_HEIGHT, Z_WIDTH)\n",
    "        with torch.inference_mode():\n",
    "            # Encoding Condition\n",
    "            self.cond_encoder.to(device)\n",
    "            if do_cfg:\n",
    "                cond_tokens = torch.tensor(tokenizer.batch_encode_plus([prompt], padding='max_length', max_length=77).input_ids, dtype=torch.long, device=device)\n",
    "                uncond_tokens = torch.tensor(tokenizer.batch_encode_plus([uncond_promt], padding='max_length', max_length=77).input_ids, dtype=torch.long, device=device)\n",
    "    \n",
    "                context = torch.cat([cond_tokens, uncond_tokens], dim=0)\n",
    "                context_embedding = self.cond_encoder(context)\n",
    "    \n",
    "            else:\n",
    "                cond_tokens = torch.tensor(tokenizer.batch_encode_plus([prompt], padding='max_length', max_length=77).input_ids, dtype=torch.long, device=device)\n",
    "                context_embedding = self.cond_encoder(cond_tokens)\n",
    "                \n",
    "            self.cond_encoder.to('cpu')\n",
    "    \n",
    "            self._set_inference_step(inference_steps)\n",
    "    \n",
    "            # Encoding Image\n",
    "            self.vae.to(device)\n",
    "            if input_image:\n",
    "                input_image = input_image.resize(IMG_HEIGHT, IMG_WIDTH)\n",
    "                input_image = np.array(input_image)\n",
    "                input_image = torch.from_array(input_image, dtype=torch.float32, device=device)\n",
    "                input_image = input_image.unsqueeze(0)\n",
    "                input_image = input_image.permute(0, 3, 1, 2)\n",
    "                \n",
    "                transformed_img = transforms(input_image)\n",
    "                latent_features =  self.vae.encode(transformed_img)\n",
    "    \n",
    "                self.set_strength(strength=strength)\n",
    "                latent_features = self.forward_process(latent_features, self.timesteps[0])\n",
    "                \n",
    "            else:\n",
    "                latent_features = torch.randn(z_shape, dtype=torch.float32, device=device)\n",
    "            self.vae.to('cpu')\n",
    "\n",
    "            # Denoising\n",
    "            timesteps = tqdm(self.timesteps.to(device))\n",
    "            # x_t: torch.Tensor, \n",
    "            # timestep: int, \n",
    "            # model_output=torch.Tensor\n",
    "    \n",
    "            self.unet.to(device)\n",
    "            for i, timestep in enumerate(timesteps):\n",
    "                # (b, 8, latent_height, latent_width)\n",
    "                model_input = latent_features\n",
    "                if do_cfg:\n",
    "                    model_input = model_input.repeat(2, 1, 1, 1)\n",
    "\n",
    "                pred_noise = self.unet(model_input, timestep, context_embedding)\n",
    "                \n",
    "                if do_cfg:\n",
    "                    cond_output, uncond_output = pred_noise.chunk(2)\n",
    "                    pred_noise = cfg_scale * (cond_output - uncond_output) + uncond_output\n",
    "    \n",
    "                latent_features = self.reverse_process(latent_features, timestep, pred_noise)\n",
    "            self.unet.to('cpu')\n",
    "    \n",
    "            self.vae.to(device)\n",
    "            generated_imgs = self.vae.decode(latent_features)\n",
    "            self.vae.to('cpu')\n",
    "    \n",
    "            generated_imgs = denormalize_img(generated_imgs, (-1, 1), (0, 255), clamp=True)\n",
    "            generated_imgs = generated_imgs.permute(0, 2, 3, 1)\n",
    "            generated_imgs = generated_imgs.to('cpu', torch.uint8).numpy()\n",
    "            return generated_imgs[0]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bd4f0cb-9313-443d-af07-dcd991862400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dnnhhuy/Study/pytorch-course/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|                                                                                                   | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                   | 0/50 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pred_alpha_hat_t' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m StableDiffusion()\n\u001b[0;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muncond_promt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muncond_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdo_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcfg_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstrength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLIPTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./weights/clip/tokenizer_vocab.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./weights/clip/tokenizer_merges.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/diffusion.py:150\u001b[0m, in \u001b[0;36mStableDiffusion.generate\u001b[0;34m(self, input_image, transforms, prompt, uncond_promt, do_cfg, cfg_scale, device, strength, inference_steps, tokenizer)\u001b[0m\n\u001b[1;32m    147\u001b[0m         cond_output, uncond_output \u001b[38;5;241m=\u001b[39m pred_noise\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    148\u001b[0m         pred_noise \u001b[38;5;241m=\u001b[39m cfg_scale \u001b[38;5;241m*\u001b[39m (cond_output \u001b[38;5;241m-\u001b[39m uncond_output) \u001b[38;5;241m+\u001b[39m uncond_output\n\u001b[0;32m--> 150\u001b[0m     latent_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Study/pytorch-course/stable-diffusion-dnnhhuy/models/diffusion.py:68\u001b[0m, in \u001b[0;36mStableDiffusion.reverse_process\u001b[0;34m(self, x_t, timestep, model_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m alpha_hat_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_hat[t, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(x_t\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     67\u001b[0m prev_alpha_hat_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphas_hat[prev_t] \u001b[38;5;28;01mif\u001b[39;00m prev_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m pred_alpha_hat_t \u001b[38;5;241m=\u001b[39m \u001b[43mpred_alpha_hat_t\u001b[49m\u001b[38;5;241m.\u001b[39mto(x_t\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt(alpha_t) \u001b[38;5;241m*\u001b[39m (x_t \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_t)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha_hat_t) \u001b[38;5;241m*\u001b[39m model_output)\n\u001b[1;32m     72\u001b[0m stdev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'pred_alpha_hat_t' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from models.diffusion import StableDiffusion\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "prompt = \"A cat stretching on the floor, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14\n",
    "\n",
    "## IMAGE TO IMAGE\n",
    "\n",
    "input_image = None\n",
    "# Comment to disable image to image\n",
    "# image_path = \"../images/dog.jpg\"\n",
    "# input_image = Image.open(image_path)\n",
    "# Higher values means more noise will be added to the input image, so the result will further from the input image.\n",
    "# Lower values means less noise is added to the input image, so output will be closer to the input image.\n",
    "strength = 0.9\n",
    "\n",
    "## SAMPLER\n",
    "num_inference_steps = 50\n",
    "seed = 4\n",
    "\n",
    "model = StableDiffusion()\n",
    "\n",
    "output = model.generate(input_image=None,\n",
    "                        prompt=prompt,\n",
    "                        uncond_promt=uncond_prompt,\n",
    "                        do_cfg=do_cfg,\n",
    "                        cfg_scale=cfg_scale,\n",
    "                        device='mps',\n",
    "                        strength=strength,\n",
    "                        inference_steps=num_inference_steps,\n",
    "                        transforms=transforms.ToTensor(),\n",
    "                        tokenizer=CLIPTokenizer('./weights/clip/tokenizer_vocab.json', merges_file='./weights/clip/tokenizer_merges.txt')\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02831304-7feb-4acc-a4aa-62c087181d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cade8a2-eba3-4ba7-a9e0-20894199d614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
